---
title: '【机器学习笔记】：模型性能度量'
date: 2024-04-17
permalink: /posts/2024/04/blog-model-performance-evaluation/
tags:
  - 机器学习
  - 西瓜书
  - 算法
---
<img src='/images/blog/2024-model-performance-evaluation/test.jpeg'>

*本文主要内容，来自于本人在阅读周志华《机器学习》[1]这本书过程中，总结出来的重要知识点。*

在分类问题中，把分类错误的样本数占样本总数的比例称为“错误率（error rate）”，而分类正确的样本数占样本总数的比例则为“精度（accuracy）”，显然有“精度 + 错误率 = 1”。更一般地，我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差（error）“，学习器在训练集上的误差称为“训练误差（training error）”或“经验误差（empirical error）“，在新样本上的误差称为“泛化误差（generalization error）”。

我们希望学习器可以从训练样本中学习到适用于所有潜在样本的“普遍规律”，这样在遇到新样本的时候，学习器才能有很好的表现，即我们的目标是减小”泛化误差“。然而，有时候，在训练过程中，学习器把训练样本的所有特征都”记住“了，但是，遇到新样本时，却无法给出正确的判断，这种现象叫做”过拟合（overfitting）“。与”过拟合“相对的是”欠拟合（underfitting）“，即学习器没有从训练样本中总结出”普遍规律“。举个极端一点的例子，两个学生A和B参加考试，A很勤奋，记忆里很好，他把以前所有的考试题目的答案都背了下来，但是A最大的缺点就是脑子比较古板，不懂得变通，题目稍微改一下，他就束手无策了。而B则不同，他很懒，记忆里也不好，考试全靠蒙。想象一下，在一场考试中，如果题目全是从过去的考卷中摘抄来的，那么A肯定可以拿满分，而B靠着蒙，也能得到一些分数；如果所有的题目都是新题目，那么A只能拿0分，B却仍然可以靠蒙，得到一些分数。A同学的学习就是“过拟合”，B同学的学习就是“欠拟合”。

<img src='/images/blog/2024-model-performance-evaluation/model-performance-evaluation-2.webp'>

严格意义上，我们是无法得到学习器的“泛化误差“的，但是可以通过实验对它进行评估。为此，需使用一个“测试集（testing set）”来测试学习器对新样本的判别能力，然后以测试集上的“测试误差（testing error）“作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。

模型评估
======
给定一个数据集$$D = \{(x_1, y_1), (x_2, y_2),...,(x_m, y_m)\}$$。如何将它划分为训练集和测试集，以及如何用他们来训练、评估学习器具呢？下面介绍了几种常用的做法：留出法、交叉验证法、自助法。

留出法
------
“留出法（hold-out）”直接将数据集$$D$$划分为两个互斥的集合，其中一个集合作为训练集$$S$$，另一个作为测试集$$T$$。需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。以最简单的二分类问题为例，假设$$D$$中正例占80%，反例占20%，那么在训练集$$S$$和测试集$$T$$中，正/反例所占百分比也应当保持一致。值得注意的是，即使保持训练集、测试集中各类样本比例一致，仍然会有多种不同的划分方式。一般来说，在使用留出法时，要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。
