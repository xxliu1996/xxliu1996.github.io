---
title: '【机器学习笔记】：模型性能度量'
date: 2024-04-17
permalink: /posts/2024/04/blog-model-performance-evaluation/
tags:
  - 机器学习
  - 西瓜书
  - 算法
---
<img src='/images/blog/2024-model-performance-evaluation/model-performance-evaluation-1.jpeg'>

*本文主要内容，来自于本人在阅读周志华《机器学习》[1]这本书过程中，总结出来的重要知识点。*

在分类问题中，把分类错误的样本数占样本总数的比例称为“错误率（error rate）”，而分类正确的样本数占样本总数的比例则为“精度（accuracy）”，显然有“精度 + 错误率 = 1”。更一般地，我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差（error）“，学习器在训练集上的误差称为“训练误差（training error）”或“经验误差（empirical error）“，在新样本上的误差称为“泛化误差（generalization error）”。

我们希望学习器可以从训练样本中学习到适用于所有潜在样本的“普遍规律”，这样在遇到新样本的时候，学习器才能有很好的表现，即我们的目标是减小”泛化误差“。然而，有时候，在训练过程中，学习器把训练样本的所有特征都”记住“了，但是，遇到新样本时，却无法给出正确的判断，这种现象叫做”过拟合（overfitting）“。与”过拟合“相对的是”欠拟合（underfitting）“，即学习器没有从训练样本中总结出”普遍规律“。举个极端一点的例子，两个学生A和B参加考试，A很勤奋，记忆里很好，他把以前所有的考试题目的答案都背了下来，但是A最大的缺点就是脑子比较古板，不懂得变通，题目稍微改一下，他就束手无策了。而B则不同，他很懒，记忆里也不好，考试全靠蒙。想象一下，在一场考试中，如果题目全是从过去的考卷中摘抄来的，那么A肯定可以拿满分，而B靠着蒙，也能得到一些分数；如果所有的题目都是新题目，那么A只能拿0分，B却仍然可以靠蒙，得到一些分数。A同学的学习就是“过拟合”，B同学的学习就是“欠拟合”。

<img src='/images/blog/2024-model-performance-evaluation/model-performance-evaluation-2.webp'>

严格意义上，我们是无法得到学习器的“泛化误差“的，但是可以通过实验对它进行评估。为此，需使用一个“测试集（testing set）”来测试学习器对新样本的判别能力，然后以测试集上的“测试误差（testing error）“作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。

模型评估
======
给定一个数据集$$D = \{(x_1, y_1), (x_2, y_2),...,(x_m, y_m)\}$$。如何将它划分为训练集和测试集，以及如何用他们来训练、评估学习器具呢？下面介绍了几种常用的做法：留出法、交叉验证法、自助法。

留出法
------
“留出法（hold-out）”直接将数据集$$D$$划分为两个互斥的集合，其中一个集合作为训练集$$S$$，另一个作为测试集$$T$$。需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似。以最简单的二分类问题为例，假设$$D$$中正例占80%，反例占20%，那么在训练集$$S$$和测试集$$T$$中，正/反例所占百分比也应当保持一致。值得注意的是，即使保持训练集、测试集中各类样本比例一致，仍然会有多种不同的划分方式。一般来说，在使用留出法时，要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如进行100次随机划分，每次产生一个训练/测试集用于实验评估，100次后就得到100个结果，而留出法返回的则是这100个结果的平均。

交叉验证法
------
“交叉验证法（cross validation）”，又叫“k折交叉验证（k-fold cross validation）”，先将数据集$$D$$划分为k个大小相似的互斥子集， $$D = D_1 \cup D_2 \cup ... \cup D_k$$。每个子集都尽可能保持数据分布的一致性。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。 

与留出法相似，将数据集$$D$$划分为k个子集同样存在多种划分方式。为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的有“10次10折交叉验证”。

<img src='/images/blog/2024-model-performance-evaluation/model-performance-evaluation-3.webp'>

自助法
------
给定包含m个样本的数据集$$D$$，我们对它进行采样产生数据集$$D'$$：每次随机从$$D$$中挑选一个样本，将其拷贝放入$$D'$$，然后再将该样本放回初始数据集$$D$$中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集$$D'$$。于是，可以把$$D'$$用作训练集，而$$D-D'$$用作测试集合。这样做的好处是，实际评估的模型与期望评估的模型都使用m个训练样本，不足之处是，$$D$$中有一部分样本会在$$D'$$中多次出现，而另一部分样本不出现，改变了初始数据集的分布。

参数调节
------
大多数机器学习算法都有一些参数需要设定，参数的设置回影响学习器的性能。机器学习通常涉及两类参数：超参数和模型参数。超参数一般由人工设定的，在设定的过程中，往往会对每个参数选择若干个候选值，通过实验来得到最佳的参数组合。而模型参数则是通过对数据的学习来确定的。

性能度量
======
如何评估学习器的性能呢？在预测任务中，要评估学习器的性能需要将学习器预测的结果与真实的标记进行比较。例如，回归任务中常用的均方误差（mean square error）通过如下公式来计算：

$$E(f; D) = \frac{1}{m} \sum^{m}_{i = 1}{(f(x_i) - y_i)}^2$$

下面介绍分类任务中常用的几种性能度量。

错误率与精度
------
错误率是分类错误的样本数占样本总数的比例，而精度则是分类正确的样本数占样本总数的比例。即有：

$$E(f; D) = \frac{1}{m} \sum^{m}_{i = 1}{{II(f(x_i) \ne y_i)}}$$

$$acc(f; D) = \frac{1}{m} \sum^{m}_{i = 1}{{II(f(x_i) = y_i)}} = 1 - E(f; D)$$

这里$$II(true) = 1$$， $$II(false) = 0$$。

查全率、查全率与$$F1$$
------
首先介绍混淆矩阵（confusion matrix）。在二分类任务中，通过比较样本数据的真实类别与学习器预测的类别来将其划分为：真正例（TP = true positive）、假正例（FP = false positive）、真反例（TN = true negatove）、假反例（FN = false negative）。